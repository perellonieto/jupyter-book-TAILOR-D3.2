# Explainable AI

<!--- This is a comment -->

**Explainable AI** (often shortened to **XAI**) is one of the ethical dimensions that is studied in the <a href="https://tailor-network.eu/">TAILOR project</a>.
The origin of XAI dates back to the entering into force of the General Data Protection Regulation (GDPR).
The GDPR {cite}`gdpr`, in its Recital 71, also mentions the right to explanation, as a suitable safeguard to ensure a fair and transparent processing in respect of data subjects. While privacy and data protection are not novel concepts, and a lot of scientific literature has been explored on these topics, the study of explainability is a new challenge.

So far, the usage of black boxes in AI and ML processes implied the possibility of inadvertently making wrong decisions due to a systematic bias in training data collection. Several practical examples have been provided, highlighting the “bias in, bias out” concept. One of the most famous examples of this concept regards a classification task: the algorithm goal was to distinguish between photos of Wolves and Eskimo Dogs (huskies) {cite}`ribeiro`. Here, the training phase of the process was done with 20 images, hand selected such that all pictures of wolves had snow in the background, while pictures of huskies did not. This choice was intentional because it was part of a social experiment. In any case, on a collection of additional 60 images, the classifier predicts “Wolf” if there is snow (or light background at the bottom), and “Husky” otherwise, regardless of animal color, position, pose, etc (see an example in Fig. {numref}`{number} <husky>`).

```{figure} ./T3.1_husky.png
---
name: husky
width: 600px
align: center
---

Raw data and explaination of a bad model's prediction in the "Husky vs Wolf" task {cite}`ribeiro`.
```

However, one of the most worrisome cases was discovered and published by ProPublica, an independent, nonprofit newsroom that produces investigative journalism with moral force. In {cite}`propublica`, the authors showed how software can actually be racist. In a nutshell, the authors analyzed a tool called COMPAS (which stands for Correctional Offender Management Profiling for Alternative Sanctions). COMPAS tries to predict, among other indexes, the recidivism of defendants, who are ranked low, medium or high risk. It was used in many US states (such as New York and Wisconsin), to suggest to judges an appropriate probation or treatment plan for individuals being sentenced. Indeed, the tool was quite accurate (around 70 percent overall with 16,000 probationers), but ProPublica journalists found that black defendants were far more likely than white defendants to be incorrectly judged to be at a higher risk of recidivism, while white defendants were more likely than black defendants to be incorrectly flagged as low risk.

From the above examples, it appears evident that explanation technologies can help companies for creating safer, more trustable products, and better managing any possible liability they may have.

The definitions contained in this book will help in understanding better the {doc}`./T3.1/XAI_dimensions` of explainations (e.g., we can discriminate between {doc}`./T3.1/model_specific`), the requirements to provide good explainations, the problems related to the Explainability topic, and the possible solutions we can adopt.


**UMBERTO**



```{bibliography}
:style: unsrt
:filter: docname in docnames
```

---

This entry was readapted from *Pratesi, Trasarti, Giannotti. Ethics in Smart Information Systems. Policy Press (currently under review)* by Francesca Pratesi.


<!---
{footcite}`propublica`
```{footbibliography}
```
for the biblography

[^note]
[^note]: This is a note
-->


