# Safety and Robustness

## In Brief

**Safety and Robustness**: The safety of an AI system refers to the
extent the system meets its intended functionality without producing any
physical or psychological harm, especially to human beings, and by
extension to other material or immaterial elements that may be valuable
for humans, including the system itself. Safety must also cover the way
and conditions in which the system ceases its operation, and the
consequences of stopping. The term robustness emphasises that safety and
---conditionally to it--- functionality, must be preserved under harsh
conditions, including unanticipated errors, exceptional situations,
unintended or intended damage, manipulation or catastrophic states.

## Abstract
<!-- bold terms in this section were <span style="color: darkblue"> -->
In this part we will cover the main elements that define the safety and robustness of AI systems. Some of them are common to system safety in general, to software-hardware computer systems or to critical systems engineering, such as **software bugs**. Some others are magnified in artificial intelligence, such as **denial of service**, a robustness issue that can appear by inducing an AI system to unrecoverable states or by generating inputs that collapse the system due to high computational demands. Some other issues are more specific to AI systems, such as **reward hacking**. These new issues appear more clearly in those systems that are specified in non-programmatic or non-explicit ways (e.g., through a utility function to be optimised, through examples, rewards or other implicit ways), as exemplified by systems that operate with solvers or machine learning models. We will pay more attention to these more AI-specific issues because they are less covered in the traditional literature about safety in computer systems. They are also more challenging because of their cognitive character, the ambiguities of human intent, several ethical issues and the relevance of long-term risks. This character and the fast development of the field has also blurred some distinctions between safety (threats without malicious intent) and {doc}`./T3.2/security` (intentional threats), especially in now popular research areas such as {doc}`./T3.2/adversarial_attack` and {doc}`data_poisoning`, and also within [data privacy](./T3.5.md) (e.g., **information leakage** by querying machine learning models or other **side channel attacks**). In the end, protecting the environment from the system (safety) also requires protecting the system from the environment ({doc}`./T3.2/security`). Taking into account the changing character of the field, we include a taxonomic organisation of terms in the area of AI safety and robustness and their definition.

## Motivation and Background 

Given the increasing capabilities and widespread use of artificial
intelligence, there is a growing concern about its risks, as humans are
progressively replaced or sidelined from the decision loop of
intelligent machines. The technical foundations and assumptions on which
traditional safety engineering principles are based are inadequate for
systems in which AI algorithms, and in particular Machine Learning (ML)
algorithms, are interacting with people and the environment at
increasingly higher levels of autonomy. There have been regulatory
efforts to limit the use of AI systems in safety-critical or hostile
environments, such as health, defense, energy, etc.
{cite}`floridi2021european, veale2021demystifying`, but the consequences can
also be devastating in areas that were not considered high risk, just by
the scaling numbers or domino effects of AI systems. On top of the
numerous safety challenges posed by present-day AI systems, a
forward-looking analysis on more capable future AI systems raises more
systemic concerns, such as highly disruptive scenarios in the workplace,
the effect on human cognition in the long term and even existential
risks.

## Guidelines

Actions to ensure safety and robustness of AI systems need to take a
holistic perspective, encompassing all the elements and stages
associated with the conception, design, implementation and maintenance
of these systems. We organise<!--[^1]--> the field of AI safety and robustness
into seven groups, following similar categorisations[^landscape]:

-   **AI Safety Foundations**: This category covers a number of foundational
    concepts, characteristics and problems related to AI safety that
    need special consideration from a theoretical perspective. This
    includes concepts such as uncertainty, generality or value
    alignment, as well as characteristics such autonomy levels, safety
    criticality, types of human-machine and environment-machine
    interaction. This group intends to collect any cross-category
    concerns in AI Safety and Robustness.

-   **Specification and Modelling**: The main scope of this category is on
    how to describe needs, designs and actual operating AI systems from
    different perspectives (technical concerns) and abstraction levels.
    This includes the specification and modelling of risk management
    properties (e.g., hazards, failures modes, mitigation measures), as
    well as safety-related requirements, training, behaviour or quality
    attributes in AI-based systems.

-   **Verification and Validation**: This category concerns design and
    implementation-time approaches to ensure that an AI-based system
    meets its requirements (verification) and behaves as expected
    (validation). The range of techniques covers any
    formal/mathematical, model-based simulation or testing approach that
    provides evidence that an AI-based system satisfies its defined
    (safety) requirements and does not deviate from its intended
    behaviour and causes unintended consequences, even in extreme and
    unanticipated situations (robustness).

-   **Runtime Monitoring and Enforcement**: The increasing autonomy and
    learning nature of AI-based systems is particularly challenging for
    their verification and validation (V&V), due to our inability to
    collect an epistemologically sufficient quantity of evidence to
    ensure correctness. Runtime monitoring is useful to cover the gaps
    of design-time V&V by observing the internal states of a given
    system and its interactions with external entities, with the aim of
    determining system behaviour correctness or predicting potential
    risks. Enforcement deals with runtime mechanisms to self-adapt,
    optimise or reconfigure system behaviour with the aim of supporting
    fallback to a safe system state from the (anomalous) current state.

-   **Human-Machine Interaction**: As autonomy progressively substitutes
    cognitive human tasks, some kind of human-machine interaction issues
    become more critical, such as the loss of situational awareness or
    overconfidence. Other issues include: collaborative missions that
    need unambiguous communication to manage self-initiative to start or
    transfer tasks; safety-critical situations in which earning and
    maintaining trust is essential at operational phases; or cooperative
    human-machine decision tasks where understanding machine decisions
    are crucial to validate safe autonomous actions.

-   **Process Assurance and Certification**: Process Assurance is the
    planned and systematic activities that assure system lifecycle
    processes conform to its requirements (including safety) and quality
    procedures. In our context, it covers the management of the
    different phases of AI-based systems, including training and
    operational phases, the traceability of data and artefacts, and
    people. Certification implies a (legal) recognition that a system or
    process complies with industry standards and regulations to ensure
    it delivers its intended functions safely. Certification is
    challenged by the inscrutability of AI-based systems and the
    inability to ensure functional safety under uncertain and
    exceptional situations prior to its operation.

-   **Safety-related Ethics, Security and Privacy**: While these are quite
    large fields, we are interested in their intersection and
    dependencies with safety and robustness. Ethics becomes increasingly
    important as autonomy (with learning and adaptive abilities)
    involves the transfer of safety risks, responsibility, and
    liability, among others. AI-specific security and privacy issues
    must be considered with regard to its impact on safety and
    robustness. For example, malicious adversarial attacks can be
    studied with focus on situations that compromise systems towards a
    dangerous situation.

Fig. {numref}`{number} <T3.2taxonomy>` reflects the seven categories described above.
Many of the terms and concepts we will expand on correspond to one or
more of these categories.


```{figure} ./T3.2/taxonomy.jpg
---
name: T3.2taxonomy
width: 600px
align: center
---
Taxonomy of AI Safety. Taken from {cite}`espinoza2019`-
```

## Recommended reading

Some introductory sources for AI Safety and Robustnes are {cite}`amodei2016,espinoza2019,gabriel2020artificial,david2019understanding,russell2015research`.

## Bibliography

```{bibliography}
:style: unsrt
:filter: docname in docnames
```

---

This entry was readapted from *Huáscar Espinoza, Han Yu, Xiaowei Huang, Freddy Lecue, José Hernández-Orallo, Seán Ó hÉigeartaigh, and Richard Mallah. Towards an AI safety landscape: an overview. Artificial Intelligence Safety 2019, https://www.ai-safety.org/.* by Jose Hernandez-Orallo, Fernando Martinez-Plumed, Santiago Escobar, and Pablo A. M. Casares.

---

<!-- http://ceur-ws.org/Vol-2419/ -->
<!--[^1]: Most of this section is taken from {cite}`espinoza2019`.-->

[^landscape]: FLI's Landscape of AI Safety and Beneficence Research for research
    contextualization and in preparation for brainstorming at the
    Beneficial AI 2017 conference
    (<https://futureoflife.org/landscape/ResearchLandscapeExtended.pdf>),
    the Assuring Autonomy International Programme (AAIP) to develop a
    Body of Knowledge (BoK) intended, in time, to become a reference
    source on assurance and regulation of Robotics and Autonomous
    Systems (RAS),
    (<https://www.york.ac.uk/assuring-autonomy/research/body-of-knowledge/>)
    and Ortega et al (DeepMind) structure of the technical AI safety
    field
    (<https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1>).
